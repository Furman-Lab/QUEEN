{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs_McpgCGvxm"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers\n",
        "!pip install fair-esm\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import os\n",
        "import os.path\n",
        "import torch\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "from datetime import date\n",
        "import torch\n",
        "import re\n",
        "import requests\n",
        "from tqdm.auto import tqdm\n",
        "import sys\n",
        "import esm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "from transformers import EsmTokenizer, EsmForSequenceClassification\n",
        "\n",
        "! git clone https://github.com/Furman-Lab/QUEEN/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
        "model = EsmForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", problem_type=\"multi_label_classification\")"
      ],
      "metadata": {
        "id": "XwiUhC4XLiHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = model.to(device)\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "8d8D7rJdMKmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ESM-2 model\n",
        "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()  # disables dropout for deterministic results"
      ],
      "metadata": {
        "id": "ISVgGw8rMOVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Few sequences predictions**"
      ],
      "metadata": {
        "id": "pMRm7EzYBWWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### Cell for embedding a few sequences\n",
        "# Prepare data (example with first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
        "# Paste your fasta sequences instead of the given sequence for your use\n",
        "data = [\n",
        "    (\"protein1\", \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"),\n",
        "    (\"protein2\", \"KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE\"),\n",
        "]\n",
        "\n",
        "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
        "token_representations = results[\"representations\"][33]\n",
        "\n",
        "\n",
        "# Extract per-residue representations (on CPU)\n",
        "sequence_representations = []\n",
        "for i, tokens_len in enumerate(batch_lens):\n",
        "  print(i)\n",
        "  print(tokens_len)\n",
        "  print(batch_lens)\n",
        "  # average on the protein length, to obtain a single vector per fasta\n",
        "  sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "\n",
        "with open(\"drive/MyDrive/ESM_embed.pkl\", \"wb\") as f:\n",
        "  pickle.dump(sequence_representations, f)\n"
      ],
      "metadata": {
        "id": "J_BXK_epvKOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If not previous tab, we generate it here:\n",
        "full_tab_for_embed = pd.DataFrame()\n",
        "np_list = []\n",
        "# Detach the tensors to obtain a numpy array\n",
        "for i, ten in enumerate(sequence_representations):\n",
        "  ten=ten.detach().numpy()\n",
        "  np_list.append(ten)\n",
        "full_tab_for_embed[\"esm_embeddings\"] = pd.Series(np_list)"
      ],
      "metadata": {
        "id": "ceJyuMe6yc-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model_location = \"QUEEN/QUEEN_MLPmodel_final.pkl\"\n",
        "with open(model_location, \"rb\") as f:\n",
        "  QUEEN_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "PBgjdgfu0shu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = QUEEN_model.predict(full_tab_for_embed[\"esm_embeddings\"].to_list())\n",
        "inv_map = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 10, 9: 12, 10: 14, 11: 24}\n",
        "y_test_transformed = np.array([inv_map[x] for x in y_test])\n",
        "print(\"These are the predicted labels:\")\n",
        "print(y_test_transformed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGi5zwEl60x2",
        "outputId": "7e12ac5f-d581-42fd-d246-4977d2a55e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These are the predictions:\n",
            "[ 3  6  4  4  4  4  4  1  4 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict sequences from a table of fastas**"
      ],
      "metadata": {
        "id": "k1RYL5YUBrQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert your table here, if you have more than a few fasta files you wish to embed\n",
        "full_tab_for_embed = pd.read_csv(\"QUEEN/Supplementary_Table_I_embed_tab_with_sets.tsv\", sep='\\t')\n",
        "full_tab_for_embed.reset_index(inplace=True, drop=True)\n",
        "# comment out the next two lines when using your table, they are here for the demo\n",
        "###\n",
        "full_tab_for_embed.drop(\"esm_embeddings\", axis=1, inplace=True)\n",
        "full_tab_for_embed = full_tab_for_embed.head(15)\n",
        "###\n",
        "print(full_tab_for_embed)"
      ],
      "metadata": {
        "id": "TZ5BQsWXxzxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this cell together with the previous one, if you have more than a few fasta sequences\n",
        "fasta_list = list(zip(full_tab_for_embed[\"fasta\"].index,full_tab_for_embed[\"fasta\"]))\n",
        "def divide_chunks(fasta_list, n):\n",
        "    # looping till length l\n",
        "    for i in range(0, len(fasta_list), n):\n",
        "        yield fasta_list[i:i + n]\n",
        "\n",
        "# if not os.path.exists(\"/embeds\"):\n",
        "#   os.makedirs(\"/embeds\")\n",
        "\n",
        "list_of_chunks = list(divide_chunks(fasta_list, 10))\n",
        "for chunk_num, chunk in enumerate(list_of_chunks):\n",
        "  fname = \"QUEEN/embed_pkl_chunk\" + str(chunk_num)\n",
        "  if os.path.isfile(fname):\n",
        "    continue\n",
        "\n",
        "\n",
        "  data = chunk\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      results = model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][33]\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "      # print(i)\n",
        "      # print(tokens_len)\n",
        "      # print(batch_lens)\n",
        "\n",
        "  del results\n",
        "  del token_representations\n",
        "  print(\"saving \" + str(chunk_num))\n",
        "  with open(fname, 'wb') as f:\n",
        "      pickle.dump(sequence_representations, f)\n"
      ],
      "metadata": {
        "id": "IvWPzsofwHvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When you do have a previous table that was used to generate the embeddings, we open all the tensors and add them to the table\n",
        "from natsort import humansorted\n",
        "esm_embeds_dir = \"QUEEN\"\n",
        "dir_list = humansorted(os.listdir(esm_embeds_dir))\n",
        "pickle_list = [x for x in dir_list if \"chunk\" in x]\n",
        "tensor_list = []\n",
        "for file in pickle_list:\n",
        "  full_path = \"QUEEN\" + \"/\" + file\n",
        "  with open(full_path, \"rb\") as f:\n",
        "    current = pickle.load(f)\n",
        "#        print(current)\n",
        "    tensor_list.extend(current)\n",
        "#        print(tensor_list)\n",
        "with open (\"QUEEN/tensor_list.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tensor_list, f)\n",
        "\n",
        "np_list = []\n",
        "for i, ten in enumerate(tensor_list):\n",
        "  ten=ten.detach().numpy()\n",
        "  np_list.append(ten)\n",
        "\n",
        "\n",
        "full_tab_for_embed[\"esm_embeddings\"] = pd.Series(np_list)\n",
        "\n",
        "with open (\"QUEEN/tab_for_pred.pkl\", \"wb\") as f:\n",
        "    pickle.dump(full_tab_for_embed, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "oa_o8ryzzeCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "model_location = \"QUEEN/QUEEN_MLPmodel_final.pkl\"\n",
        "with open(model_location, \"rb\") as f:\n",
        "  QUEEN_model = pickle.load(f)"
      ],
      "metadata": {
        "id": "VwreHA5I8h9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = QUEEN_model.predict(full_tab_for_embed[\"esm_embeddings\"].to_list())\n",
        "inv_map = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8, 8: 10, 9: 12, 10: 14, 11: 24}\n",
        "y_test_transformed = np.array([inv_map[x] for x in y_test])\n",
        "full_tab_for_embed[\"y_pred\"] = pd.Series(y_test_transformed)\n",
        "print(\"These are the predicted labels:\")\n",
        "print(y_test_transformed)\n",
        "print(\"this is the final table\")\n",
        "print(full_tab_for_embed)"
      ],
      "metadata": {
        "id": "cniGd9-A6fe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMknvNwj8tvk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
